# -*- coding: utf-8 -*-
"""sentiment-analysis-WAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XTFwoV75ozgZxOMjUin-8HrHUikk0t0x
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/Rajesh-M01/Sentiment-Analysis-APP/main/sentimentanalysistest.csv", encoding="ISO-8859-1")


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.model_selection import train_test_split as tts
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,classification_report

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Bidirectional, Dense, Dropout,Input
from tensorflow.keras.models import Model

df=df[["text", "sentiment"]]
print(df.head())

def clean_text(text):
  text=str(text).lower()
  text=re.sub(r"https\S+", "", text)
  text=re.sub(r"@\w+", "", text)
  text = re.sub(r"[^a-zA-Z0-9.,!?()']", " ", text)
  text=re.sub(r"\s+", " ", text).strip()
  return text

import string

df["cleaned_text"]=df["text"].apply(clean_text)
df = df.dropna(subset=["cleaned_text", "sentiment"])
print(df.columns)
sentiment_counts=df["sentiment"].value_counts()
print(sentiment_counts)


df.head()

import seaborn as sns
plt.figure(figsize=(5,5))
sns.countplot(x=df["sentiment"], palette='Set2')
plt.title("Class Distribution")
plt.show()

#Shows that there is imbalance

# Upsampling done wrt Positive & Negative classes to match Neutral class to counter the imbalance as seen in the last graph diagram
from sklearn.utils import resample

df_neutral = df[df["sentiment"] == "neutral"]
df_positive = df[df["sentiment"] == "positive"]
df_negative = df[df["sentiment"] == "negative"]

df_positive_upsampled = resample(df_positive,
                                 replace=True,  # Allow duplicates
                                 n_samples=len(df_neutral),
                                 random_state=42)

df_negative_upsampled = resample(df_negative,
                                 replace=True,
                                 n_samples=len(df_neutral),
                                 random_state=42)

df_balanced = pd.concat([df_neutral, df_positive_upsampled, df_negative_upsampled])
df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

print(df_balanced["sentiment"].value_counts())  # Check new class distribution

import seaborn as sns
plt.figure(figsize=(5,5))
sns.countplot(x=df["sentiment"], palette='Set2')
plt.title("Class Distribution")
plt.show()

#Shows that there is no imbalance

vectorizer=TfidfVectorizer(stop_words=None, max_features=25000, ngram_range=(1,2), min_df=3, max_df=0.8)
x_tfidf=vectorizer.fit_transform(df["cleaned_text"])


x_train_tfidf, x_test_tfidf, y_train, y_test = tts(x_tfidf, df["sentiment"], test_size=0.2, random_state=0)

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
x_train_tfidf, y_train = smote.fit_resample(x_train_tfidf, y_train)

print("TF-IDF feauture extraction complete!")

for a in [0.001, 0.005, 0.01, 0.05, 0.1]:
    nb_model = MultinomialNB(alpha=a)
    nb_model.fit(x_train_tfidf, y_train)
    y_pred_nb = nb_model.predict(x_test_tfidf)
    print(f"Alpha {a} - Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}")


print("\nNaive Bayes Accuracy: ", accuracy_score(y_test, y_pred_nb)*100)
print("\nNaive Bayes Classification Report:\n ", classification_report(y_test, y_pred_nb))

from transformers import BertTokenizer, TFBertForSequenceClassification

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

from sklearn.model_selection import train_test_split as tts

print(df["sentiment"].unique())


label_mapping = {"positive": 2, "neutral": 1, "negative": 0}
df["sentiment_label"] = df["sentiment"].map(label_mapping)

print(df["sentiment_label"].unique())

from sklearn.utils import resample

df_majority = df[df['sentiment_label'] == 2]
df_minority = df[df['sentiment_label'] != 2]

df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)
df_balanced = pd.concat([df_majority, df_minority_upsampled])

x_train, x_test, y_train, y_test = tts(df_balanced["cleaned_text"], df_balanced["sentiment_label"], test_size=0.2, random_state=42)

y_train_bert = np.array(y_train, dtype=np.int32)
y_test_bert = np.array(y_test, dtype=np.int32)


print("Sample encoded y_train:", y_train_bert[:5])

# Ensure x_train is a list of strings and has no NaN values
x_train = x_train.dropna().astype(str).tolist()
x_test = x_test.dropna().astype(str).tolist()

def encode_text(texts, tokenizer, max_length=128):
    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors="tf")

# Now encode properly
train_encodings = encode_text(x_train, tokenizer)
test_encodings = encode_text(x_test, tokenizer)
def encode_text(texts, tokenizer, max_length=128):
    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors="tf")


train_encodings = encode_text(x_train, tokenizer)
test_encodings = encode_text(x_test, tokenizer)

train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),y_train_bert )). batch(8)
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),y_test_bert )). batch(8)

import tensorflow as tf
import keras

print("TensorFlow Version:", tf.__version__)  # Should be 2.15.0
print("Keras Version:", keras.__version__)  # Should be 2.15.0

import os
os.environ["KERAS_BACKEND"] = "tensorflow"

import numpy as np


preds = bert_model.predict(test_dataset).logits
predicted_labels = np.argmax(preds, axis=1)


y_test_np = np.concatenate([y.numpy() for _, y in test_dataset], axis=0)

for i in range(min(10, len(y_test_np))):
    if predicted_labels[i] != y_test_np[i]:
        print(f"Text: {x_test[i]}")
        print(f"Actual: {y_test_np[i]}, Predicted: {predicted_labels[i]}\n")

from sklearn.utils import class_weight

class_weights = class_weight.compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train),
    y=y_train
)
class_weights = {i: weight for i, weight in enumerate(class_weights)}

from transformers import create_optimizer
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=1000)

bert_model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=["accuracy"])
bert_model.fit(train_dataset, validation_data=test_dataset, epochs=8, class_weight=class_weights)

loss, accuracy = bert_model.evaluate(test_dataset)
print(f"Final BERT Test Accuracy: {accuracy * 100:.2f}%")

import numpy as np


preds = bert_model.predict(test_dataset).logits
predicted_labels = np.argmax(preds, axis=1)


y_test_np = np.concatenate([y.numpy() for _, y in test_dataset], axis=0)


for i in range(min(10, len(y_test_np))):  # Displays 10 incorrect predictions
    if predicted_labels[i] != y_test_np[i]:
        print(f"Text: {x_test[i]}")
        print(f"Actual: {y_test_np[i]}, Predicted: {predicted_labels[i]}\n")

bert_model.save_pretrained("bert_sentiment_model")
tokenizer.save_pretrained("bert_sentiment_model")

from transformers import TFBertForSequenceClassification, BertTokenizer

model = TFBertForSequenceClassification.from_pretrained("bert_sentiment_model")
tokenizer = BertTokenizer.from_pretrained("bert_sentiment_model")

def predict_sentiment(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors="tf", padding=True, truncation=True, max_length=128)
    logits = model(inputs["input_ids"]).logits
    predicted_class = np.argmax(logits, axis=1)[0]
    sentiment_mapping = {0: "Negative", 1: "Neutral", 2: "Positive"}
    return sentiment_mapping[predicted_class]

text = "I love this product! It's amazing."
print("Predicted Sentiment for",text," -> ", predict_sentiment(text, bert_model, tokenizer), "\n")

text1="Disgusting Product ! Would never recommend it."
print("Predicted Sentiment for",text1," -> ", predict_sentiment(text1, bert_model, tokenizer), "\n" )

text2="It's decent ! Not Bad Not Good"
print("Predicted Sentiment for",text2," -> ", predict_sentiment(text2, bert_model, tokenizer), "\n" )

from sklearn.metrics import classification_report


preds = bert_model.predict(test_dataset).logits
predicted_labels = np.argmax(preds, axis=1)


y_test_np = np.concatenate([y.numpy() for _, y in test_dataset], axis=0)


report = classification_report(y_test_np, predicted_labels, target_names=["Negative", "Neutral", "Positive"])
print(report)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


cm = confusion_matrix(y_test_np, predicted_labels)


plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Negative", "Neutral", "Positive"], yticklabels=["Negative", "Neutral", "Positive"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("BERT Sentiment Analysis - Confusion Matrix")
plt.show()



#WEB DEPLOYMENT

import streamlit as st
from transformers import TFBertForSequenceClassification, BertTokenizer
import numpy as np


@st.cache_resource
def load_model():
    model = TFBertForSequenceClassification.from_pretrained("bert_sentiment_model")
    tokenizer = BertTokenizer.from_pretrained("bert_sentiment_model")
    return model, tokenizer

model, tokenizer = load_model()


def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="tf", padding=True, truncation=True, max_length=128)
    logits = model(inputs["input_ids"]).logits
    sentiment_mapping = {0: "Negative", 1: "Neutral", 2: "Positive"}
    return sentiment_mapping[np.argmax(logits)]


st.title("üî• Sentiment Analysis Web App üî•")
st.write("Enter a sentence below to analyze its sentiment.")

user_input = st.text_area("Enter text here:")
if st.button("Predict Sentiment"):
    if user_input.strip():
        sentiment = predict_sentiment(user_input)
        st.success(f"Predicted Sentiment: **{sentiment}**")
    else:
        st.warning("‚ö†Ô∏è Please enter some text before predicting!")


